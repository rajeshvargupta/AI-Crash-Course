{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM+VVQxkMe4QB+7/NKXEZmo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Decision Tree\n","\n","<center><img src='https://i.imgur.com/Tup5Lpc.jpg'></center>\n","\n","A decision tree is a flowchart-like tree structure where an internal node represents feature(or attribute), the branch represents a decision rule, and each leaf node represents the outcome. The topmost node in a decision tree is known as the root node. It learns to partition on the basis of the attribute value. It partitions the tree in recursively manner call recursive partitioning. This flowchart-like structure helps you in decision making. It's visualization like a flowchart diagram which easily mimics the human level thinking. That is why decision trees are easy to understand and interpret.\n","\n","**Decision tree is a type of supervised learning algorithm that can be used in both regression and classification problems. It works for both categorical and continuous input and output variables.**\n","\n","\n","\n","Let's identify important terminologies on Decision Tree, looking at the image above:\n","\n","- **Root Node:** It represents the entire population or sample. It further gets divided into two or more homogeneous sets.\n","\n","- **Splitting:** Splitting is a process of dividing a node into two or more sub-nodes.\n","\n","- **Decision Node:** When a sub-node splits into further sub-nodes, it is called a Decision Node.\n","\n","- **Terminal Node or a Leaf Node:** Nodes that do not split is called a Terminal Node or a Leaf.\n","\n","- **Pruning:** When you remove sub-nodes of a decision node, this process is called Pruning. The opposite of pruning is Splitting.\n","=\n","- **Branch:** A sub-section of an entire tree is called Branch.\n","\n","- A node, which is divided into sub-nodes is called a **parent node** of the sub-nodes; whereas the sub-nodes are called **the child** of the parent node.\n","\n","\n","## Types of Decision Trees \n","\n","### Regression Trees\n","\n","Let's take a look at the image below, which helps visualize the nature of partitioning carried out by a Regression Tree. \n","\n","<center><img src='https://i.imgur.com/Iw4Z0gH.jpg'></center>\n","\n","This shows an unpruned tree and a regression tree fit to a random dataset. Both the visualizations show a series of splitting rules, starting at the top of the tree. Notice that every split of the domain is aligned with one of the feature axes. The concept of axis parallel splitting generalises straightforwardly to dimensions greater than two. For a feature space of size $p$, a subset of $\\mathbb{R}^p$, the space is divided into $M$ regions, $R_{m}$, each of which is a $p$-dimensional \"hyperblock\".\n","\n","<center><img src='https://i.imgur.com/TPUkbEG.jpg'></center>\n","\n","In order to build a regression tree, you first use recursive binary splititng to grow a large tree on the training data, stopping only when each terminal node has fewer than some minimum number of observations. Recursive Binary Splitting is a greedy and top-down algorithm used to minimize the Residual Sum of Squares (RSS), an error measure also used in linear regression settings. The RSS, in the case of a partitioned feature space with M partitions is given by:\n","\n","Beginning at the top of the tree, you split it into 2 branches, creating a partition of 2 spaces. You then carry out this particular split at the top of the tree multiple times and choose the split of the features that minimizes the (current) RSS.\n","\n","Next, you apply cost complexity pruning to the large tree in order to obtain a sequence of best subtrees, as a function of $\\alpha$. The basic idea here is to introduce an additional tuning parameter, denoted by $\\alpha$ that balances the depth of the tree and its goodness of fit to the training data.\n","\n","You can use K-fold cross-validation to choose $\\alpha$. This technique simply involves dividing the training observations into K folds to estimate the test error rate of the subtrees. Your goal is to select the one that leads to the lowest error rate.\n","\n","\n","### Classification Trees\n","\n","A classifiction tree is very similar to a regression tree, except that it is used to predict a qualitative response rather than a quantitative one.\n","\n","<center><img src='https://i.imgur.com/ySLNjqa.jpg'></center>\n","\n","Recall that for a regression tree, the predicted response for an observation is given by the mean response of the training observations that belong to the same terminal node. In contrast, for a classification tree, you predict that each observation belongs to the most commonly occurring class of training observations in the region to which it belongs.\n","\n","In interpreting the results of a classification tree, you are often interested not only in the class prediction corresponding to a particular terminal node region, but also in the class proportions among the training observations that fall into that region.\n","\n","\n","\n","The task of growing a classification tree is quite similar to the task of growing a regression tree. Just as in the regression setting, you use recursive binary splitting to grow a classification tree. However, in the classification setting, Residual Sum of Squares cannot be used as a criterion for making the binary splits. Instead, you can use either of these 3 methods below:\n","\n","**Classification Error Rate:** Rather than seeing how far a numerical response is away from the mean value, as in the regression setting, you can instead define the \"hit rate\" as the fraction of training observations in a particular region that don't belong to the most widely occuring class. The error is given by this equation:\n","\n","E = 1 - argmaxc($\\hat{\\pi}_{mc}$)\n","\n","in which $\\hat{\\pi}_{mc}$ represents the fraction of training data in region $R_m$ that belong to class c."],"metadata":{"id":"zvnfTukvT8P7"}},{"cell_type":"markdown","source":["## Assumptions\n","\n","Despite such simplicity of a decision tree, it holds certain assumptions like:\n","\n","- Discretization of continuous variables is required\n","- The data taken for training should be wholly considered as root\n","- Distribution of records is done in a recursive manner on the basis of attribute values."],"metadata":{"id":"kkxezHinbRus"}},{"cell_type":"markdown","source":["## Purity in Decision Tree\n","\n","A leaf of a tree that contains data points and it shares the same target values is called **pure**.\n","\n","### Node Impurity\n","\n","<center><img src='https://i.imgur.com/RXjmjFK.png'></center>\n","\n","**Pure :** Pure means, in a selected sample of dataset all data belongs to same class (PURE).\n","\n","**Impure :** Impure means, data is mixture of different classes.\n","\n","Node impurity is the homogeneity within a node. A node is impure if cases have more than one value for the response. A node is pure if all instances have the same value for the response or target variable or impurity = 0.\n","\n","These are the two most popular methods for measuring node impurity:\n","\n","- **Entropy**\n","- **Gini Index**\n","\n","**The best split is selected based on the degree of impurity of the child nodes**. Node impurity is 0 when all patterns at the node are of the same category. Impurity becomes maximum when all the classes at node N are equally likely."],"metadata":{"id":"-hccL49rcOa8"}},{"cell_type":"markdown","source":["### Entropy\n","\n","**In a decision tree, entropy is a kind of disorder or uncertainty.** It is the measure of impurity, disorder, or uncertainty in a bunch of data. It is a way to control the split of data decided by a decision tree. It influences how a decision tree forms its boundaries. We use entropy to measure the impurity or randomness of a dataset.\n","\n","\n","\n","Consider a dataset with N classes. The entropy may be calculated using the formula below:\n","\n","$E=−∑_{i=1}^Np_ilog_2p_i$\n","\n","$p_i$ is the probability of randomly selecting an example in class i.\n","\n","\n","Let’s have an example to better our understanding of entropy and its calculation. \n","\n","Let’s have a dataset made up of three colors; red, purple, and yellow. \n","\n","If we have one red, three purple, and four yellow observations in our set, our equation becomes:\n","\n","$E=−(p_rlog_2p_r+p_plog_2p_p+p_ylog_2p_y)$\n","\n","Where $p_r, p_p$ and $p_y$ are the probabilities of choosing a red, purple and yellow example respectively. \n","\n","We have $p_r$=18 because only 18 of the dataset represents red. \n","\n","38 of the dataset is purple hence $p_p$=38. \n","\n","Finally, $p_y$=48 since half the dataset is yellow. \n","\n","As such, we can represent $p_y$ as $p_y$=12\n","\n","Our equation now becomes:\n","\n","$E=−(18log_2(18)+38log_2(38)+48log_2(48))$\n","\n","**Our entropy would be: 1.41**\n","\n","\n","\n","You might wonder, what happens when all observations belong to the same class? In such a case, the entropy will always be zero.\n","\n","**$E=−(1log_21)$**\n","\n","**=0** \n","\n","Such a dataset has no impurity. This implies that such a dataset would not be useful for learning. \n","\n","However, if we have a dataset with say, two classes, **half made up of yellow and the other half being purple, the entropy will be one**.\n","\n","**$E=−((0.5log_20.5)+(0.5log_20.5))$**\n","\n","**=1**\n","\n"],"metadata":{"id":"k5Su4teVezn1"}},{"cell_type":"markdown","source":["<center><img src='https://i.imgur.com/ncYIfL4.png'></center>"],"metadata":{"id":"_PllLsRqyYLh"}},{"cell_type":"markdown","source":["### Splitting\n","\n","**Attribute Selection Measures**\n","\n","Attribute selection measure is a heuristic for selecting the splitting criterion that partition data into the best possible manner. It is also known as splitting rules because it helps us to determine breakpoints for tuples on a given node. \n","\n","ASM provides a rank to each feature(or attribute) by explaining the given dataset. Best score attribute will be selected as a splitting attribute. In the case of a continuous-valued attribute, split points for branches also need to define. Most popular selection measures are **Information Gain, Gain Ratio, and Gini Index.**\n","\n"],"metadata":{"id":"lWo5SV0td8wX"}},{"cell_type":"markdown","source":["### Information Gain\n","\n","Shannon invented the concept of entropy, which measures the impurity of the input set. \n","\n","In physics and mathematics, entropy referred as the randomness or the impurity in the system. \n","\n","In information theory, it refers to the impurity in a group of examples. \n","\n","**Information gain is the decrease in entropy**. \n","\n","Information gain computes **the difference between entropy before split and average entropy after split of the dataset based on given attribute values.** \n","\n","**ID3 (Iterative Dichotomiser)** decision tree algorithm uses information gain.\n","\n","Information Gain measures how much information a feature provides about the class.\n","<center><img src='https://i.imgur.com/94yv9OM.jpg'></center>\n","\n","Information Gain is significant in a Decision tree due to the points below:\n","\n","- It is the primary key accepted by the Decision tree algorithm to build a Decision tree.\n","- The Decision Tree will evermore try to maximize information gain.\n","- **The attribute which has the highest information gain will be tested or split first.**\n","\n","### **Information Gain = Entropy(parent) - Weighted Sum of Entropy(Children)**\n","\n","\n","[Refer this link to get more clarity](https://www.analyticsvidhya.com/blog/2021/03/how-to-select-best-split-in-decision-trees-using-information-gain/)\n","\n"],"metadata":{"id":"kNLjHKuAegzn"}},{"cell_type":"markdown","source":[],"metadata":{"id":"-thtwFZaqmAH"}},{"cell_type":"markdown","source":["### Gini\n","\n","Like entropy,the Gini index is also a type of criterion in decision trees that serves to calculate information gain. Information gain is used by the decision tree to split a node. **Gini measures the impurity of a node.**\n","\n","The range of Gini lies between **0 to 0.5**. \n","\n","Gini impurity is better compared to entropy for selecting the best features.\n","\n","The equation of the Gini index to measure impurity:\n","\n","$Gini= 1−∑_{i=1}^N{p_i}^2$\n","\n","The probability of samples belonging to class $i$ at a given node can be denoted as $p_i$\n","\n","**An attribute with the smallest Gini Impurity is selected for splitting the node.**\n","\n","[Refer this link to get more clarity](https://www.analyticsvidhya.com/blog/2021/03/how-to-select-best-split-in-decision-trees-gini-impurity/)\n"],"metadata":{"id":"tTiTYM5XkScs"}},{"cell_type":"markdown","source":["<center><img src='https://i.imgur.com/1KEvmqo.png'></center>"],"metadata":{"id":"F7j_rtHHyinY"}},{"cell_type":"markdown","source":["### Entropy Vs Gini Index\n","\n","<center><img src='https://i.imgur.com/KE1yQfu.png'></center>\n","\n","The Gini Index and the Entropy have two main differences:\n","\n","- Gini Index has values inside the interval [0, 0.5] whereas the interval of the Entropy is [0, 1]. In the following figure, both of them are represented. The gini index has also been represented multiplied by two to see concretely the differences between them, which are not very significant.\n","\n","- Computationally, entropy is more complex since it makes use of logarithms and consequently, the calculation of the Gini Index will be faster.\n","\n","As you can see in the graph for entropy, it first increases up to 1 and then starts decreasing, but in the case of Gini impurity it only goes up to 0.5 and then it starts decreasing, hence it requires less computational power. The range of Entropy lies in between 0 to 1 and the range of Gini Impurity lies in between 0 to 0.5."],"metadata":{"id":"2d3_cjx78JWY"}},{"cell_type":"markdown","source":["### Chi Square\n","Chi-square is another method of splitting nodes in a decision tree for datasets having categorical target values. It can make two or more than two splits. It works on the statistical significance of differences between the parent node and child nodes.\n","\n","Chi-Square value is:\n","\n","<center><img src='https://i.imgur.com/hnWQi63.jpg'></center>\n","\n","\n","\n","Here, the Expected is the expected value for a class in a child node based on the distribution of classes in the parent node, and Actual is the actual value for a class in a child node.\n","\n","The above formula gives us the value of Chi-Square for a class. Take the sum of Chi-Square values for all the classes in a node to calculate the Chi-Square for that node. Higher the value, higher will be the differences between parent and child nodes, i.e., higher will be the homogeneity.\n","\n","Here are the steps to split a decision tree using Chi-Square:\n","\n","1. For each split, individually calculate the Chi-Square value of each child node by taking the sum of Chi-Square values for each class in a node\n","2. Calculate the Chi-Square value of each split as the sum of Chi-Square values for all the child nodes\n","3. Select the split with higher Chi-Square value\n","4. Until you achieve homogeneous nodes, repeat steps 1-3\n","\n","[Refer this explaining Chi-Square in the context of a decision tree](https://www.analyticsvidhya.com/blog/2021/03/how-to-select-best-split-in-decision-trees-using-chi-square/)"],"metadata":{"id":"2oqVYptptCE4"}},{"cell_type":"markdown","source":["### Reduction in Variance\n","\n","Reduction in Variance is a method for splitting the node used when the target variable is continuous, i.e., regression problems. It is so-called because it uses variance as a measure for deciding the feature on which node is split into child nodes.\n","\n","<center><img src='https://i.imgur.com/cBGDSU9.jpg'></center>\n","\n","Variance is used for calculating the homogeneity of a node. If a node is entirely homogeneous, then the variance is zero.\n","\n","Here are the steps to split a decision tree using reduction in variance:\n","\n","1. For each split, individually calculate the variance of each child node\n","2. Calculate the variance of each split as the weighted average variance of child nodes\n","3. Select the split with the lowest variance\n","4. Perform steps 1-3 until completely homogeneous nodes are achieved\n","\n","[Refere this link for more clarity](https://www.saedsayad.com/decision_tree_reg.htm)\n"],"metadata":{"id":"z_t1GRKbuHJJ"}},{"cell_type":"markdown","source":["## Types of Decision Tree Algorithms\n","\n","Hunt’s algorithm, which was developed in the 1960s to model human learning in Psychology, forms the foundation of many popular decision tree algorithms, such as the following: \n","\n","- **ID3:** Ross Quinlan is credited within the development of ID3, which is shorthand for “Iterative Dichotomiser 3.” This algorithm leverages entropy and information gain as metrics to evaluate candidate splits. Some of Quinlan’s research on this algorithm from 1986 can be found here (PDF, 1.4 MB) (link resides outside of ibm.com).\n","\n","- **C4.5:** This algorithm is considered a later iteration of ID3, which was also developed by Quinlan. It can use information gain or gain ratios to evaluate split points within the decision trees. \n","\n","- **CART:** The term, CART, is an abbreviation for “classification and regression trees” and was introduced by Leo Breiman. This algorithm typically utilizes Gini impurity to identify the ideal attribute to split on. Gini impurity measures how often a randomly chosen attribute is misclassified. When evaluating using Gini impurity, a lower value is more ideal. \n","\n","- **CHAID:** CHAID stands for Chi-square Automatic Interaction Detector. It is known to be the oldest of all three algorithms in history and is used very less these days. In CHAID chi-square is the attribute selection measure to split the nodes when it's a classification-based use case and uses F-test as an attribute selection measure when it is a regression-based use case. Higher the chi-square value higher is the preference given to that feature. The major difference between CHAID and CART is, CART splits one node into two nodes whereas CHAID splits one node into 2 or more nodes.\n","\n","- **MARS:** MARS stands for Multivariate adaptive regression splines. It is an algorithm that was specifically designed to handle regression-based tasks, provided, the data is non-linear.\n","\n"],"metadata":{"id":"gNGT7JDbw4sK"}},{"cell_type":"markdown","source":["## Pruning\n","\n","Pruning is a technique that removes the parts of the Decision Tree which prevent it from growing to its full depth. The parts that it removes from the tree are the parts that do not provide the power to classify instances. A Decision tree that is trained to its full depth will highly likely lead to overfitting the training data - therefore Pruning is important. \n","\n","In simpler terms, the aim of Decision Tree Pruning is to construct an algorithm that will perform worse on training data but will generalize better on test data. Tuning the hyperparameters of your Decision Tree model can do your model a lot of justice and save you a lot of time and money. \n","\n"," \n","### How do you Prune a Decision Tree?\n","\n","There are two types of pruning: Pre-pruning and Post-pruning. I will go through both of them and how they work.\n","\n"," \n","**1. Pre-pruning**\n","\n"," \n","\n","The pre-pruning technique of Decision Trees is tuning the hyperparameters prior to the training pipeline. It involves the heuristic known as ‘early stopping’ which stops the growth of the decision tree - preventing it from reaching its full depth. \n","\n","It stops the tree-building process to avoid producing leaves with small samples. During each stage of the splitting of the tree, the cross-validation error will be monitored. If the value of the error does not decrease anymore - then we stop the growth of the decision tree. \n","\n","The hyperparameters that can be tuned for early stopping and preventing overfitting are:\n","\n","**max_depth, min_samples_leaf, and min_samples_split**  \n","\n","These same parameters can also be used to tune to get a robust model. However, you should be cautious as early stopping can also lead to underfitting.\n","\n"," \n","**2. Post-pruning**\n","\n"," \n","\n","Post-pruning does the opposite of pre-pruning and allows the Decision Tree model to grow to its full depth. Once the model grows to its full depth, tree branches are removed to prevent the model from overfitting. \n","\n","The algorithm will continue to partition data into smaller subsets until the final subsets produced are similar in terms of the outcome variable. The final subset of the tree will consist of only a few data points allowing the tree to have learned the data to the T. However, when a new data point is introduced that differs from the learned data - it may not get predicted well. \n","\n","The hyperparameter that can be tuned for post-pruning and preventing overfitting is: ccp_alpha\n","\n","ccp stands for Cost Complexity Pruning and can be used as another option to control the size of a tree. A higher value of ccp_alpha will lead to an increase in the number of nodes pruned.\n","\n","Cost complexity pruning (CCP) (post-pruning) steps:\n","\n","- Train your Decision Tree model to its full depth\n","- Compute the ccp_alphas value using cost_complexity_pruning_path()\n","- Train your Decision Tree model with different ccp_alphas values and compute train and test performance scores\n","- Plot the train and test scores for each value of ccp_alphas values. \n","\n","This hyperparameter can also be used to tune to get the best fit models.\n","\n","#### Minimal cost-complexity pruning\n","Minimal cost-complexity pruning is an algorithm used to prune a tree to avoid over-fitting .This algorithm is parameterized by $α>=0$ known as the complexity parameter. The complexity parameter is used to define the cost-complexity measure,$R_α(T)$ of a given tree $T$ :\n","\n","$R_α(T)=R(T)+α|T̃|$\n","\n","where \n","\n","$|T̃|$ is the number of terminal nodes in T and R(T) is traditionally defined as the total misclassification rate of the terminal nodes. Alternatively, scikit-learn uses the total sample weighted impurity of the terminal nodes for R(T) . As shown above, the impurity of a node depends on the criterion. Minimal cost-complexity pruning finds the subtree of  T that minimizes $R_α(T)$.\n","\n","The cost complexity measure of a single node is $R_α(t)=R(t)+α$\n","\n","The branch, $T_t$ , is defined to be a tree where node is its root. In general, the impurity of a node is greater than the sum of impurities of its terminal nodes,$R(T_t) < R(t)$ . \n","\n","However, the cost complexity measure of a node,t , and its branch,$T_t$ , can be equal depending on $α$. \n","\n","We define the effective $α$ of a node to be the value where they are equal, $R_α(t)=R(t)$ or $α_{eff}(t)==  \\frac {R(t)-R(T_t)}{|T|-1}$\n",". \n","\n","A non-terminal node with the smallest value of $α_{eff}$ is the weakest link and will be pruned. This process stops when the pruned tree’s minimal $α_{eff}$ is greater than the `ccp_alpha` parameter."],"metadata":{"id":"tUadpDdD0Ux4"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vd1pbcqnT4BV"},"outputs":[],"source":[]}]}
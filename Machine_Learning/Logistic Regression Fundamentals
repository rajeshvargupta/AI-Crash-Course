{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyMx6fzj4Hi9AeTxpZVtwPk2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Logistic Regression\n","\n","Logistic regression is a machine learning algorithm used for classification problems.  The term logistic is derived from the cost function (logistic function) which is a type of sigmoid function known for its characteristic S-shaped curve.  A logistic regression model predicts probability values which are mapped to two (binary classification) or more (multiclass classification) classes.\n","\n","<center><img src='https://i.imgur.com/L4LweK7.png'></center>\n","\n","\n","<center><img src='https://i.imgur.com/Exx3qeT.png'> </center>\n","\n"],"metadata":{"id":"xaY9WOKx2DhA"}},{"cell_type":"markdown","source":["## Assumptions of Logistic Regression\n","\n","1. Logistic regression requires the dependent variable to be discrete mostly dichotomous i.e. binary.\n","\n","2. Since logistic regression estimates the probability of the event occurring (P(Y=1)), it is necessary to code the dependent variable accordingly. That is the desired out-come should be coded to be 1.\n","\n","3. The model should be fitted correctly. It should not be over fitted with the meaningless variables included. Also it should not be under fitted with meaningful variable not included.\n","\n","4. Logistic regression requires each observation to be independent. Also the model should have little or no multicollinearity. That is, independent variables are not linear functions of each other.\n","\n","5. Whilst logistic regression does not require a linear relationship be-\n","tween the dependent and independent variables, it requires that the in-\n","dependent variables are linearly related to the log odds of an event.\n","\n","6. Logistic regression requires large sample sizes because maximum likelihood estimates are less powerful than ordinary least squares used for estimating the unknown parameters in a linear regression model.\n"],"metadata":{"id":"oUX2R6ZkHEk2"}},{"cell_type":"markdown","source":["## Binary Classification\n","\n","**What are the odds?**\n","Odds are defined as the ratio of the probability of an event occurring to the probability of the event not occurring.\n","\n","For Example, let’s assume that the probability of winning a game is 0.02. Then, the probability of not winning is 1- 0.02 = 0.98.\n","\n","- The odds of winning the game= (Probability of winning)/(probability of not winning)\n","    \n","- The odds of winning the game= 0.02/0.98\n","\n","- The odds of winning the game are 1 to 49, and the odds of not winning the game are 49 to 1.\n","\n","###  Logistic Regression Formula\n","\n","Logistic Regression is a special case of linear regression where the target variable is categorical in nature. It uses a log of odds as the dependent variable. Logistic Regression predicts the probability of occurrence of a binary event utilizing a logit function.\n","\n","**Linear Regression Equation:**\n","\n","## $y=β_0+β_1 X_1+............+β_n X_n$\n","\n","Where, $y$ is a dependent variable and $X_1, X_2 ...$ and $X_n$ are explanatory variables.\n","\n","**Logit equation:**\n","\n","## $log(\\frac p{1+p} )=y=β_0+β_1 X_1+............+β_n X_n$\n","\n","**Sigmoid Function:**\n","\n","## $p=\\frac 1{1+e^{-y}}$\n","\n","**Apply Sigmoid function on linear regression:**\n","\n","## $p=\\frac 1{1+e^{-(β_0+β_1 X_1+............+β_n X_n)}}$\n","\n","\n","The sigmoid function, also called logistic function gives an ‘S’ shaped curve that can take any real-valued number and map it into a value between 0 and 1.\n","\n","<center><img src='https://i.imgur.com/YUyj4MU.png'></center>\n","\n","**If the curve goes to positive infinity, y predicted will become 1, and if the curve goes to negative infinity, y predicted will become 0.**\n","\n","**If the output of the sigmoid function is more than 0.5, we can classify the outcome as 1 or YES, and if it is less than 0.5, we can classify it as 0 or NO.**\n","\n","For example: If the output is 0.75, we can say in terms of probability as: There is a 75 percent chance that a patient will suffer from cancer."],"metadata":{"id":"KaVkV_6lLnep"}},{"cell_type":"markdown","source":["### Types of Logistic Regression\n","\n","Types of Logistic Regression:\n","\n","1. **Binary Logistic Regression:** The target variable has only two possible outcomes such as Spam or Not Spam, Cancer or No Cancer.\n","2. **Multinomial Logistic Regression:** The target variable has three or more nominal categories such as predicting the type of Wine.\n","3. **Ordinal Logistic Regression:** the target variable has three or more ordinal categories such as restaurant or product rating from 1 to 5.\n"],"metadata":{"id":"gGu5bD8qjt2V"}},{"cell_type":"markdown","source":["## Performance Matrix For Logistic Regression\n","\n","Binary classification has four possible types of results:\n","\n","- True negatives: correctly predicted negatives (zeros)\n","- True positives: correctly predicted positives (ones)\n","- False negatives: incorrectly predicted negatives (zeros)\n","- False positives: incorrectly predicted positives (ones)\n","\n","You usually evaluate the performance of your classifier by comparing the actual and predicted outputsand counting the correct and incorrect predictions.\n","\n","The most straightforward indicator of classification accuracy is the ratio of the number of correct predictions to the total number of predictions (or observations).\n","\n","Following is the list of 10 metrics which we’ll study in an interconnected way through examples:\n","\n","1. Confusion Matrix\n","2. Type I Error\n","3. Type II Error\n","4. Accuracy\n","5. Recall or True Positive Rate or Sensitivity\n","6. Precision\n","7. Specificity\n","8. F1 Score\n","9. ROC Curve- AUC Score\n","10. PR Curve"],"metadata":{"id":"-l2DL4B9ZyXI"}},{"cell_type":"markdown","source":["### 1. Confusion Matrix\n","\n","<center><img src='https://i.imgur.com/VGYBW8C.png'></center>\n","\n","We start with a development dataset while building any statistical or ML model. Divide that dataset into 2 parts: Training and Test. Keep aside the test dataset and train the model using the training dataset. Once the model is ready to predict, we try making predictions on the test dataset. And once we segment the results into a matrix similar to as shown in the above figure, we can see how much our model is able to predict right and how much of its predictions are wrong.\n","\n","We populate the following 4 cells with the numbers from our test dataset(having 1000 observations for instance).\n","\n","1. TP (True-positives): Where the actual label for that column was “Yes” in the test dataset and our logistic regression model also predicted “Yes”. (500 observations)\n","2. TN (True-negatives): Where the actual label for that column was “No” in the test dataset and our logistic regression model also predicted “No”. (200 observations)\n","3. FP (False-positives): Where the actual label for that column was “No” in the test dataset but our logistic regression model predicted “Yes”. (100 observations)\n","4. FN (False-negatives): Where the actual label for that column was “Yes” in the test dataset but our logistic regression model predicted “No”. (200 observations)\n","\n","These 4 cells constitute the “Confusion matrix” as in the matrix which can alleviate all the confusion about the goodness of our model by painting a clear picture of our model’s predictive power.\n","\n","A confusion matrix is a table that is often used to describe the performance of a classification model (or “classifier”) on a set of test data for which the true values are known.\n","\n"],"metadata":{"id":"Ly8AN8iuaxOq"}},{"cell_type":"markdown","source":["### 2. Type I Error\n","\n","<center><img src='https://i.imgur.com/S0EZo3s.png'></center>\n","\n","`A type 1 error is also known as a false positive and occurs when a classification model incorrectly predicts a true outcome for an originally false observation.`\n","\n","For example: Let’s suppose our logistic model is working on a spam-not spam email use case. If our model is labeling an otherwise important email as spam then this is an example of Type I error by our model. In this particular problem statement, we are sensitive towards reducing the Type I error as much as possible because important emails going into the spam email can have serious repercussions.\n","\n"],"metadata":{"id":"eyPYpwPQdni0"}},{"cell_type":"markdown","source":["### 3. Type II Error\n","\n","<center><img src='https://i.imgur.com/Ctv03bD.png'></center>\n","\n","`A type II error is also known as a false negative and occurs when a classification model incorrectly predicts a false outcome for an originally true observation.`\n","\n","For example: Let’s suppose our logistic model is working on a use case where it has to predict if a person has cancer or not. If our model labels a person having cancer as a healthy person and misclassifies it, then this is an example of Type II error by our model. In this particular problem statement, we are very sensitive towards reducing the Type II error as much as possible because false negatives in this scenario may lead to death if the disease goes on to be undiagnosed in an affected person.\n"],"metadata":{"id":"kUuAyAcOdyII"}},{"cell_type":"markdown","source":["### 4. Accuracy\n","\n","Now, the above three metrics discussed are General-purpose metrics irrespective of the kind of training and test data that you have and the kind of classification algorithm yu have deployed for your problem statements.\n","\n","We are now moving towards discussing the metrics which are well suited for a particular type of data.\n","\n","Let’s start talking about Accuracy here, this is a metric that is best used for a balanced dataset. Refer to the diagram below\n","\n","<center><img src='https://i.imgur.com/KgwsqvR.png'></center>\n","\n","\n","As you can see, a balanced dataset is one where the 1’s and 0’s, yes’s and no’s, positive and negatives are equally represented by the training data. On the other hand, if the ratio of the two class-labels is skewed then our model will get biased towards one category.\n","\n","Assuming we have a Balanced dataset, let’s learn what is Accuracy.\n","\n","`Accuracy is the proximity of measurement results to the true value. It tell us how accurate our classification model is able to predict the class labels given in the problem statement.`\n","\n","For example: Let’s suppose that our classification model is trying to predict for customer attrition scenario. In the image above, Out of the total 700 actually attrited customers (TP+FN) , the model was correctly able to classify 500 attrited customers correctly (TP). Similarly, out of the total 300 retained customers (FP+TN), the model was correctly able to classify 200 retained customers correctly (TN).\n","\n","**Accuracy= (TP+TN)/Total customers**\n","\n","\n","<center><img src='https://i.imgur.com/JuTQuzE.png'></center>\n","\n","\n","In the above scenario, we see that the accuracy of the model on the test dataset of 1000 customers is 70%.\n","\n","Now, we learned that Accuracy is a metric that should be used only for a balanced dataset. Why is that so? Let’s look at an example to understand that.\n","\n","<center><img src='https://i.imgur.com/q1zlfrU.png'></center>\n","\n","In this example, this model was trained on an imbalanced dataset and even the test dataset is imbalanced. The Accuracy metric has a score of 72% which might give us the impression that our model is doing a good job at the classification. But, look closer, this model is doing a terrible job out of predicting the Negative class labels. It only predicted 20 correct outcomes out of 100 total negative label observations. This is why the Accuracy metric should not be used if you have an imbalanced dataset.\n","\n","The next question is, then what is to be used if you have an imbalanced dataset? The answer is Recall and Precision. Let’s learn more about these.\n"],"metadata":{"id":"wcHOFRTvd4Wc"}},{"cell_type":"markdown","source":["### 5. Recall/ Sensitivity/ TPR\n","\n","<center><img src='https://i.imgur.com/sxCQ8X0.png'></center>\n","\n","Recall/ Sensitivity/ TPR (True Positive Rate) attempts to answer the following question:\n","\n","**What proportion of actual positives was identified correctly?**\n","\n","<center><img src='https://i.imgur.com/7anyMy6.png'></center>\n","\n","\n","\n","This metric gives us 78% as the Recall score in the above image. Recall is generally used in use cases where the truth-detection is of utmost importance. For example: The cancer prediction, the stock market classification, etc. over here the problem statement requires that the False negatives be minimized which implies Recall/Sensitivity be maximized.\n"],"metadata":{"id":"RS6pwmXNeBnQ"}},{"cell_type":"markdown","source":["### 6. Precision\n","\n","<center><img src='https://i.imgur.com/Xn6K7E0.png'></center>\n","\n","Precision attempts to answer the following question:\n","\n","**What proportion of positive identifications was actually correct?**\n","\n","<center><img src='https://i.imgur.com/TyQRvkB.png'></center>\n","\n","\n","\n","The example shown in the above image shows us that the Precision score is 75%. Precision is generally used in cases where it’s of utmost importance not to have a high number of False positives. For example: In spam detection cases, as we discussed above, a false positive would be an observation that was not spam but was classified as Spam by our classification model. Too many of the false positives can defeat the purpose of a spam classifier model. Thus, Precision comes handy here to judge the model performance in this scenario.\n"],"metadata":{"id":"jYL8vn-NeF9t"}},{"cell_type":"markdown","source":["### 7. Specificity\n","\n","<center><img src='https://i.imgur.com/MV8VU2p.png'></center>\n","\n","\n","`Specificity (also called the true negative rate) measures the proportion of actual negatives that are correctly identified as such.`\n","\n","<center><img src='https://i.imgur.com/a7cVsDA.png'></center>\n","\n","\n","\n","Building on the same spam detection classifier example which we took to understand Precision. Specificity tells us how many negatives our model was able to accurately classify. In this example, we see that specificity =33%, this is not a good score for a spam detection model as this implies majority of non-spam emails are getting wrongly classified as spam. We can derive the conclusion that this model needs improvement by looking at the specificity metric.\n"],"metadata":{"id":"QBJoiUdweJwE"}},{"cell_type":"markdown","source":["### 8. F1 Score\n","\n","We talked about Recall and Precision in points numbers 6 and 7 respectively. We understand that there are some problem statements where a higher Recall takes precedence over a higher Precision and vice-versa.\n","\n","But there are some use-cases, where the distinction is not very clear and as developers, we want to give importance to both Recall and Precision. In this case, there is another metric- F1 Score that can be used. It is dependent on both Precision and Recall.\n","\n","`In a statistical analysis of binary classification, the F1 score (also F-score or F-measure) is a measure of a test’s accuracy. It considers both the precision p and the recall r of the test to compute the score`\n","\n","<center><img src='https://i.imgur.com/S8YTHF9.png'></center>\n","\n","\n","Before moving to the last two metrics, the following is a good summary table provided on Wikipedia that covers all the metrics we have discussed until now in this post.\n","\n","<center><img src='https://i.imgur.com/uwJVuWC.png'></center>\n","\n","\n","Now, we are at the last leg of this post. Until now we have discussed the model performance metrics for classification models that predict class labels. Now, let’s study the metrics for the models which operate basis on the probabilities.\n"],"metadata":{"id":"OwsGmA8ReNrA"}},{"cell_type":"markdown","source":["### 9. ROC Curve- AUC Score\n","\n","Area under the Curve (AUC), Receiver Operating Characteristics curve (ROC)\n","\n","\n","<center><img src='https://i.imgur.com/krkDldy.png'></center>\n","\n","This is one of the most important metrics used for gauging the model performance and is widely popular among the data scientists.\n","\n","Let’s start understanding this with an example. We have a classification model that gives probability values ranging between 0–1 to predict the probability of a person being obese or not. Probability score near 0 indicates a very low probability that the person under consideration is obese whereas probability values near 1 indicate a very high probability of a person being obese. Now, by default if we consider a threshold of 0.5 then all the people assigned probabilities ≤0.5 will be classified as “Not Obese” and people assigned probabilities >0.5 will be classified as “Obese”. But, we can vary this threshold. What if I make it 0.3 or 0.9. Let’s see what happens.\n","\n","For simplicity in understanding, we have taken 10 people in our sample.\n","\n","    To plot a ROC curve, we have to plot (1-Specificity) i.e. False Positive Rate on x-axis and Sensitivity i.e. True Positive Rate on the y-axis.\n","\n","ROC (Receiver Operating Characteristic) Curve tells us about how good the model can distinguish between two things (e.g If a patient is obese or not). Better models can accurately distinguish between the two. Whereas, a poor model will have difficulties in distinguishing between the two.\n","\n","We’ll see 4 different scenarios wherein we are going to select different values of the threshold and will calculate corresponding x and y-axis values for the ROC curve.\n","\n","\n","<center> <img src='https://i.imgur.com/BVfra2f.png'>\n","\n","**Scenario 1: Threshold value=0.9**\n","\n","\n","<img src='https://i.imgur.com/OttZZQS.png'>\n","\n","**Scenario 2: Threshold value=0.6**\n","\n","<img src='https://i.imgur.com/0ek7gmg.png'>\n","\n","**Scenario 3: Threshold Value=0.3**\n","\n","<img src='https://i.imgur.com/pW5E7eQ.png'>\n","\n","**Scenario 4: Threshold value=0**</center>\n","\n","Now, we have 4 data points with the help of which we’ll plot our ROC Curve as shown below.\n","\n","\n","<center><img src='https://i.imgur.com/sIREa9Q.png'>\n","\n","<img src='https://i.imgur.com/DwaBkME.jpg'></center>\n","\n","\n","Thus, this is how ROC Curves can be plotted for a classification model by assigning its different thresholds to create different data points to generate the ROC Curve. The area under the ROC curve is known as AUC. The more the AUC the better your model is. The farther away your ROC curve is from the middle linear line, the better your model is. This is how ROC-AUC can help us judge the performance of our classification models as well as provide us a means to select one model from many classification models."],"metadata":{"id":"Tsk8PxcSeQ8e"}},{"cell_type":"markdown","source":["### 10. PR Curve\n","\n","In cases where the data is located mostly in the negative label, the ROC-AUC will give us a result that will not be able to represent the reality much because we primarily focus on a positive rate approach, TPR on y-axis and FPR on the x-axis.\n","\n","\n","\n","For instance, look at the example below:\n","\n","<center><img src='https://i.imgur.com/kqJpfDH.png'></center>\n","\n","Over here you can see that most of the data lie under the negative label and ROC-AUC will not capture that information. In these kinds of scenarios, we turn to PR curves which are nothing but the Precision-Recall curve.\n","\n","In a PR curve, we’ll calculate and plot Precision on Y-axis and Recall on X-axis to see how our model is performing.\n","\n","<center><img src='https://i.imgur.com/ZD71FBN.png'></center>"],"metadata":{"id":"hp0LBPSP_Ich"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"UWfJnLuvyNo-"},"outputs":[],"source":[]}]}
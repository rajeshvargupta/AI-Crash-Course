{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6h9y0fICAcUc"
   },
   "source": [
    "# **1. Introduction to NLP**\n",
    "![](https://i.imgur.com/aRMVbVe.jpg)\n",
    "\n",
    "**Natural language processing (NLP)** refers to the branch of computer science—and more specifically, the branch of artificial intelligence or AI—concerned with giving computers the ability to understand text and spoken words in much the same way human beings can.\n",
    "\n",
    "NLP is used to convert text from one language to another, provide a summary to a large amount of text, respond to customer queries in chatbots, digital assistants. It also found application in voice-operated GPS systems and other consumer conveniences. NLP is becoming increasingly popular in companies for providing business solutions to enhance customer experiences, streamline operations and increase profit.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uk0_X_EtBk2S"
   },
   "source": [
    "# **2. Task of NLP**\n",
    "\n",
    "The task of NLP is complex as the natural language is ambiguous and uncertain. There are different types of ambiguities present in natural language:\n",
    "\n",
    "1. **Lexical Ambiguity:** It is defined as the ambiguity associated with the meaning of a single word. A single word can have different meanings. Also, a single word can be a noun, adjective, or verb. For example, The word “bank” can have different meanings. It can be a financial bank or a riverbank. Similarly, the word “clean” can be a noun, adverb, adjective, or verb.\n",
    "\n",
    "2. **Syntactic Ambiguity:** It is defined as the ambiguity associated with the way the words are parsed. For example, The sentence “Visiting relatives can be boring.”  This sentence can have two different meanings. One is that visiting a relative’s house can be boring. The second is that visiting relatives at your place can be boring.\n",
    "\n",
    "3. **Semantic Ambiguity:** It is defined as ambiguity when the meaning of the words themselves can be ambiguous. For example, The sentence “Mary knows a little french.” In this sentence the word “little french” is ambiguous. As we don’t know whether it is about the language french or a person.\n",
    "\n",
    "\n",
    "Several NLP tasks break down human text and voice data in ways that help the computer make sense of what it's ingesting. Some of these tasks include the following:\n",
    "\n",
    "- **Speech recognition**, also called **speech-to-text**, is the task of reliably converting voice data into text data. Speech recognition is required for any application that follows voice commands or answers spoken questions. What makes speech recognition especially challenging is the way people talk—quickly, slurring words together, with varying emphasis and intonation, in different accents, and often using incorrect grammar.\n",
    "- **Part of speech tagging**, also called **grammatical tagging**, is the process of determining the part of speech of a particular word or piece of text based on its use and context. Part of speech identifies ‘make’ as a verb in ‘I can make a paper plane,’ and as a noun in ‘What make of car do you own?’\n",
    "- **Word sense disambiguation** is the selection of the meaning of a word with multiple meanings  through a process of semantic analysis that determine the word that makes the most sense in the given context. For example, word sense disambiguation helps distinguish the meaning of the verb 'make' in ‘make the grade’ (achieve) vs. ‘make a bet’ (place).\n",
    "- **Named entity recognition, or NEM**, identifies words or phrases as useful entities. NEM identifies ‘Kentucky’ as a location or ‘Fred’ as a man's name.\n",
    "- **Co-reference resolution** is the task of identifying if and when two words refer to the same entity. The most common example is determining the person or object to which a certain pronoun refers (e.g., ‘she’ = ‘Mary’),  but it can also involve identifying a metaphor or an idiom in the text  (e.g., an instance in which 'bear' isn't an animal but a large hairy person).\n",
    "- **Sentiment analysis** attempts to extract subjective qualities—attitudes, emotions, sarcasm, confusion, suspicion—from text.\n",
    "- **Natural language generation** is sometimes described as the opposite of speech recognition or speech-to-text; it's the task of putting structured information into human language. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KN8dHYSuC1bk"
   },
   "source": [
    "## 2.1. Phases of Natural Language Processing\n",
    "\n",
    "There are roughly five phases of Natural language processing:\n",
    "phases of NLP\n",
    "\n",
    "\n",
    "1. **Lexical Analysis:**\n",
    "\n",
    "The first phase is lexical analysis/morphological processing. In this phase, the sentences, paragraphs are broken into tokens. These tokens are the smallest unit of text. It scans the entire source text and divides it into meaningful lexemes. For example, The sentence “He goes to college.” is divided into [ ‘He’ , ‘goes’ , ‘to’ , ‘college’, ‘.’] . There are five tokens in the sentence. A paragraph may also be divided into sentences.\n",
    "lexical analysis\n",
    "\n",
    "2. **Syntactic Analysis/Parsing:** \n",
    "\n",
    "The second phase is Syntactic analysis. In this phase, the sentence is checked whether it is well-formed or not. The word arrangement is studied and a syntactic relationship is found between them. It is checked for word arrangements and grammar. For example, the sentence “Delhi goes to him” is rejected by the syntactic parser.\n",
    "\n",
    "3. **Semantic Analysis:**  \n",
    "\n",
    "The third phase is Semantic Analysis. In this phase, the sentence is checked for the literal meaning of each word and their arrangement together. For example, The sentence “I ate hot ice cream” will get rejected by the semantic analyzer because it doesn’t make sense.\n",
    "\n",
    "4. **Discourse Integration:** \n",
    "\n",
    "The fourth phase is discourse integration. In this phase, the impact of the sentences before a particular sentence and the effect of the current sentence on the upcoming sentences is determined.\n",
    "\n",
    "5. **Pragmatic Analysis:** \n",
    "\n",
    "The last phase of natural language processing is Pragmatic analysis. Sometimes the discourse integration phase and pragmatic analysis phase are combined. The actual effect of the text is discovered by applying the set of rules that characterize cooperative dialogues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gm4gxfhTEAFi"
   },
   "source": [
    "# **3. What is text analysis?**\n",
    "\n",
    "Text analysis is the process of using computer systems to read and understand human-written text for business insights. Text analysis software can independently classify, sort, and extract information from text to identify patterns, relationships, sentiments, and other actionable knowledge. You can use text analysis to efficiently and accurately process multiple text-based sources such as emails, documents, social media content, and product reviews, like a human would.\n",
    "\n",
    "## How does text analysis work?\n",
    "\n",
    "The core of text analysis is training computer software to associate words with specific meanings and to understand the semantic context of unstructured data. This is similar to how humans learn a new language by associating words with objects, actions, and emotions. \n",
    "\n",
    "Text analysis software works on the principles of deep learning and natural language processing.\n",
    "\n",
    "**Deep learning**\n",
    "\n",
    "Artificial intelligence is the field of data science that teaches computers to think like humans. Machine learning is a technique within artificial intelligence that uses specific methods to teach or train computers. Deep learning is a highly specialized machine learning method that uses neural networks or software structures that mimic the human brain. Deep learning technology powers text analysis software so these networks can read text in a similar way to the human brain.\n",
    "Natural language processing\n",
    "\n",
    "**Natural language processing (NLP)** \n",
    "\n",
    "It uses linguistic models and statistics to train the deep learning technology to process and analyze text data, including handwritten text images. NLP methods such as optical character recognition (OCR) convert text images into text documents by finding and understanding the words in the images.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xk5A_7ACFB9z"
   },
   "source": [
    "# **4. Tokenization**\n",
    "\n",
    "By tokenizing, you can conveniently split up text by word or by sentence. This will allow you to work with smaller pieces of text that are still relatively coherent and meaningful even outside of the context of the rest of the text. It’s your first step in turning unstructured data into structured data, which is easier to analyze.\n",
    "\n",
    "When you’re analyzing text, you’ll be tokenizing by word and tokenizing by sentence. Here’s what both types of tokenization bring to the table:\n",
    "\n",
    "1. **Tokenizing by word:** Splitting a sentence in words.\n",
    "\n",
    "```\n",
    "Hello everyone. Welcome to Party.\n",
    "\n",
    "output: ['Hello', 'everyone', '.', 'Welcome', 'to', 'Party', '.'] \n",
    "\n",
    "```\n",
    "\n",
    "2. **Tokenizing by sentence:** Splitting as sentence\n",
    "\n",
    "```\n",
    "Hello everyone. Welcome to NLP Lecture. You are studying NLP.\n",
    "\n",
    "Output: ['Hello everyone.',\n",
    " 'Welcome to NLP Lecture.',\n",
    " 'You are studying NLP.']\n",
    "\n",
    "```\n",
    "\n",
    "3. **RegexpTokenizer:** Splits the sentence into words based on regular expression. \n",
    "\n",
    "```\n",
    "tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "text = \"Let's see how it's working.\"\n",
    "\n",
    "Output: [\"Let's\", 'see', 'how', \"it's\", 'working']\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ATy7Fd-2KAV6"
   },
   "source": [
    "# **5.Stop words**\n",
    "\n",
    "Stop words are words that you want to ignore, so you filter them out of your text when you’re processing it. \n",
    "\n",
    "Very common words like 'in', 'is', and 'an' are often used as stop words since they don’t add a lot of meaning to a text in and of themselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ONLpo6mkKtRi"
   },
   "source": [
    "# **6. Stemming**\n",
    "\n",
    "Stemming is a text processing task in which you reduce words to their root, which is the core part of a word. \n",
    "\n",
    "![](https://i.imgur.com/ugo42th.jpg)\n",
    "\n",
    "For example, the words **“helping” and “helper” share the root “help.” ** \n",
    "\n",
    "Stemming allows you to zero in on the basic meaning of a word rather than all the details of how it’s being used. \n",
    "\n",
    "\n",
    "**Understemming and overstemming are two ways stemming can go wrong:**\n",
    "\n",
    "- **Understemming** happens when two related words should be reduced to the same stem but aren’t. This is a false negative.\n",
    "- **Overstemming** happens when two unrelated words are reduced to the same stem even though they shouldn’t be. This is a false positive.\n",
    "\n",
    "The Porter stemming algorithm dates from 1979, so it’s a little on the older side. The Snowball stemmer, which is also called Porter2, is an improvement on the original and is also available through NLTK, so you can use that one in your own projects. \n",
    "\n",
    "It’s also worth noting that the purpose of the Porter stemmer is not to produce complete words but to find variant forms of a word.\n",
    "\n",
    "Fortunately, you have some other ways to reduce words to their core meaning, such as lemmatizing, which you’ll see later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wyboCTAdLl9m"
   },
   "source": [
    "# **7. Lemmatizing**\n",
    "\n",
    "Now that you’re up to speed on parts of speech, you can circle back to lemmatizing. Like stemming, lemmatizing reduces words to their core meaning, but it will give you a complete English word that makes sense on its own instead of just a fragment of a word. \n",
    "\n",
    "![](https://i.imgur.com/WySc1xi.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3BSTLU25NJsv"
   },
   "source": [
    "# **8. Tagging Parts of Speech**\n",
    "\n",
    "Part of speech is a grammatical term that deals with the roles words play when you use them together in sentences. T**agging parts of speech, or POS tagging,** is the task of labeling the words in your text according to their part of speech.\n",
    "\n",
    "In English, there are eight parts of speech:\n",
    "\n",
    "![](https://i.imgur.com/cLrLYZJ.jpg)\n",
    "\n",
    "\n",
    "**Here’s a summary that you can use to get started with NLTK’s POS tags:**\n",
    "\n",
    "![](https://i.imgur.com/Ht9ct7x.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qF6Oog70N_Qk"
   },
   "source": [
    "# **9. Feature Generation using TF-IDF**\n",
    "\n",
    "In **Term Frequency(TF)**, you just count the number of words occurred in each document. The main issue with this Term Frequency is that it will give more weight to longer documents. Term frequency is basically the output of the BoW model.\n",
    "\n",
    "**IDF(Inverse Document Frequency)** measures the amount of information a given word provides across the document. IDF is the logarithmically scaled inverse ratio of the number of documents that contain the word and the total number of documents.\n",
    "function\n",
    "\n",
    "TF-IDF(Term Frequency-Inverse Document Frequency) normalizes the document term matrix. It is the product of TF and IDF. Word with high tf-idf in a document, it is most of the times occurred in given documents and must be absent in the other documents. So the words must be a signature word.\n",
    "\n",
    "![](https://i.imgur.com/U7V1mIr.jpg)\n",
    "\n",
    "where \n",
    "\n",
    "d refers to a document, \n",
    "\n",
    "N is the total number of documents, \n",
    "\n",
    "df is the number of documents with term t.\n",
    "\n",
    "## **Bag of Words (BoW) Model**\n",
    "\n",
    "The Bag of Words (BoW) model is the simplest form of text representation in numbers. Like the term itself, we can represent a sentence as a bag of words vector (a string of numbers).\n",
    "\n",
    "Let’s recall the three types of movie reviews we saw earlier:\n",
    "\n",
    "- Review 1: This movie is very scary and long\n",
    "- Review 2: This movie is not scary and is slow\n",
    "- Review 3: This movie is spooky and good\n",
    "\n",
    "We will first build a vocabulary from all the unique words in the above three reviews. The vocabulary consists of these 11 words: ‘This’, ‘movie’, ‘is’, ‘very’, ‘scary’, ‘and’, ‘long’, ‘not’,  ‘slow’, ‘spooky’,  ‘good’.\n",
    "\n",
    "We can now take each of these words and mark their occurrence in the three movie reviews above with 1s and 0s. This will give us 3 vectors for 3 reviews:\n",
    "\n",
    "![](https://i.imgur.com/2VK6OKb.jpg)\n",
    "\n",
    "\n",
    "We will again use the same vocabulary we had built in the Bag-of-Words model to show how to calculate the TF for Review #2:\n",
    "\n",
    "Review 2: This movie is not scary and is slow\n",
    "\n",
    "Here,\n",
    "\n",
    "- Vocabulary: ‘This’, ‘movie’, ‘is’, ‘very’, ‘scary’, ‘and’, ‘long’, ‘not’,  ‘slow’, ‘spooky’,  ‘good’\n",
    "- Number of words in Review 2 = 8\n",
    "- TF for the word ‘this’ = (number of times ‘this’ appears in review 2)/(number of terms in review 2) = 1/8\n",
    "\n",
    "Similarly,\n",
    "\n",
    "![](https://i.imgur.com/VpLgmYX.jpg)\n",
    "\n",
    "We can calculate the IDF values for the all the words in Review 2:\n",
    "\n",
    "IDF(‘this’) =  log(number of documents/number of documents containing the word ‘this’) = log(3/3) = log(1) = 0\n",
    "\n",
    "Similarly,\n",
    "\n",
    "![](https://i.imgur.com/zT5furz.jpg)\n",
    "\n",
    "Hence, we see that words like “is”, “this”, “and”, etc., are reduced to 0 and have little importance; while words like “scary”, “long”, “good”, etc. are words with more importance and thus have a higher value.\n",
    "\n",
    "We can now compute the TF-IDF score for each word in the corpus. Words with a higher score are more important, and those with a lower score are less important\n",
    "\n",
    "We can now calculate the TF-IDF score for every word in Review 2:\n",
    "\n",
    "TF-IDF(‘this’, Review 2) = TF(‘this’, Review 2) * IDF(‘this’) = 1/8 * 0 = 0\n",
    "\n",
    "Similarly,\n",
    "\n",
    "![](https://i.imgur.com/9y8xQoV.jpg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p-PMEnTqi5Re"
   },
   "source": [
    "# **10. Naïve Bayes Algorithm**\n",
    "\n",
    "Naive Bayes classifiers are built on Bayesian classification methods. These rely on Bayes's theorem, which is an equation describing the relationship of conditional probabilities of statistical quantities. In Bayesian classification, we're interested in finding the probability of a label given some observed features, which we can write as P(L | features). Bayes's theorem tells us how to express this in terms of quantities we can compute more directly:\n",
    "\n",
    "\n",
    "![](https://i.imgur.com/s8VSM3x.png)\n",
    "![](https://i.imgur.com/O9fMgga.png)\n",
    "\n",
    "If we are trying to decide between two labels—let's call them L1 and L2—then one way to make this decision is to compute the ratio of the posterior probabilities for each label:\n",
    "![](https://i.imgur.com/Oezwnrk.jpg)\n",
    "\n",
    "All we need now is some model by which we can compute P(features | Li) for each label. Such a model is called a generative model because it specifies the hypothetical random process that generates the data. Specifying this generative model for each label is the main piece of the training of such a Bayesian classifier. \n",
    "\n",
    "![](https://i.imgur.com/NiXudcR.png)\n",
    "\n",
    "![](https://i.imgur.com/YpLt2Rr.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DzuaP3R7-ibF"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN1leKZLMuEKZAPgTpv8cU1",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
